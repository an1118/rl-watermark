# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy
import os
import random
import time
from dataclasses import dataclass
from tqdm import tqdm
import json

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import tyro

from transformers import AutoTokenizer, AutoModelForCausalLM
from vllm import LLM, SamplingParams
from datasets import load_dataset

from models_cl import RobertaForCL
from util import vocabulary_mapping, WatermarkLogitsBias, selective_log_softmax, sign_ste, watermark_logits_bias, safe, fill_na
from text_quality_score import _judge_text_quality
from attack import paraphrase_attack, spoofing_attack, latter_spoofing_attack, hate_attack

@dataclass
class Args:
    exp_name: str = os.path.basename(__file__)[: -len(".py")]
    """the name of this experiment"""
    seed: int = 666
    """seed of the experiment"""
    torch_deterministic: bool = True
    """if toggled, `torch.backends.cudnn.deterministic=False`"""
    cuda: bool = True
    """if toggled, cuda will be enabled by default"""
    track: bool = True
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "rl-watermark"
    """the wandb's project name"""
    wandb_entity: str = None
    """the entity (team) of wandb's project"""

    # GRPO training arguments
    num_iterations: int = 200
    """the number of iterations (computed in runtime)"""
    batch_size: int = 16  # 16
    """the batch size"""
    num_minibatches: int = 2  # 1
    """the number of mini-batches"""
    eval_interval: int = 5
    """the interval of evaluation"""
    G: int = 8  # 8
    """the number of rollouts generated for each original text"""
    learning_rate: float = 1e-5
    """the learning rate of the optimizer"""
    anneal_lr: bool = False
    """Toggle learning rate annealing for policy and value networks"""
    clip_coef: float = 0.2
    """the surrogate clipping coefficient"""
    max_grad_norm: float = 0.5
    """the maximum norm for the gradient clipping"""
    # target_kl: float = None
    # """the target KL divergence threshold"""

    # Watermark specific arguments
    embed_map_model_name: str = "Shiyu-Lab/roberta-base-watermark-embed"
    """the name of the embedding model"""
    watermark_model_name: str = "meta-llama/Llama-3.1-8B-Instruct"
    """the name of the watermark model"""

    # Dataset specific arguments
    dataset_name: str = "Shiyu-Lab/C4-contrastive-watermark"
    """the name of the dataset"""
    num_paraphrased_llama: int = None
    """the number of paraphrased examples generated by Llama"""
    num_paraphrased_gpt: int = None
    """the number of paraphrased examples generated by GPT"""
    num_sentiment_spoof: int = None
    """the number of sentiment spoofing examples"""
    num_latter_sentiment_spoof: int = None
    """the number of latter sentiment spoofing examples"""
    num_hate: int = None
    """the number of hate spoofing examples"""

    # Sanity check arguments
    is_sanity_check: bool = True
    """if toggled, this experiment will be a sanity check"""

    # to be filled in runtime
    minibatch_size: int = 0
    """the mini-batch size (computed in runtime)"""
    checkpoint_dir: str = None
    """where to save best embed_map_model checkpoints"""

SYS_PROMPT = f'''Paraphrase the following text while preserving its original meaning. Ensure that the output meets the following criteria:

1. **Preserves Meaning** – The paraphrase should convey the same core idea without omitting or distorting information.
2. **Fluency and Grammar** – The paraphrase must be natural, grammatically correct, and well-structured.
3. **Appropriate Length** – Maintain a similar length unless a slight adjustment improves clarity.
4. **Consistency with Context** – Retain the original tone and formality (e.g., academic, casual, professional).
5. **Minimal Redundancy** – Avoid unnecessary repetition while keeping essential details.
6. **Retains Nuances** – Preserve connotations, implied meanings, and idiomatic expressions where appropriate.

Just provide the paraphrased version of the text, without any introductory or concluding phrases.
'''


class Actor(nn.Module):
    def __init__(self, embed_map_model_name, watermark_model_name):
        super().__init__()
        # cuda_visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES", None)
        self.gpu0 = torch.device(f"cuda:0")  # for wm model - vllm
        self.gpu1 = torch.device(f"cuda:1")  # for wm model - transformer
        # self.gpu2 = torch.device(f"cuda:{cuda_visible_devices.split(',')[2]}")  # for embed model

        self.watermark_model_vllm = LLM(
            model="meta-llama/Llama-3.1-8B-Instruct", 
            tensor_parallel_size=1,
            max_model_len=2000,
        )

        self.embed_map_tokenizer = AutoTokenizer.from_pretrained(embed_map_model_name)
        self.embed_map_model = RobertaForCL.from_pretrained(embed_map_model_name).to(self.gpu1)
        for param in self.embed_map_model.parameters():
            param.requires_grad = True
        self.watermark_tokenizer = AutoTokenizer.from_pretrained(watermark_model_name)
        self.watermark_model = AutoModelForCausalLM.from_pretrained(watermark_model_name).to(self.gpu1)
        for param in self.watermark_model.parameters():
            param.requires_grad = False  # freeze the watermark model

        vocabulary_size = self.watermark_model.config.vocab_size
        self.mapping_list = vocabulary_mapping(vocabulary_size, 384, seed=66)

        self.delta = 0.13  # watermark strength
        self.alpha = 1.0  # entropy threshold to add watermark
        self.measure_threshold = 20  # threshold to measure the entropy of the logits

        hate_phrases_path = "hate_phrase.json"
        with open(hate_phrases_path, 'r') as f:
            self.hate_phrases_list = json.load(f)

    def rollout(self, text, G):
        # get G/R split
        green_red_split = self._get_green_red_split(text)

        # add prompt instruction
        messages = [
            {
                "role": "system", "content": SYS_PROMPT,
            },
            {
                "role": "user",  "content": text
            },
        ]
        prompt = self.watermark_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        # generate watermarked text
        logits_processors = [WatermarkLogitsBias(green_red_split, self.alpha, self.delta)]
        sampling_params = SamplingParams(
            n=G,
            top_p=0.9,
            max_tokens=500,
            logprobs=0,  # only return logprobs for the generated tokens
            logits_processors=logits_processors,
        )
        outputs = self.watermark_model_vllm.generate([prompt], sampling_params, use_tqdm=False)
        ## save output results
        watermarked_texts, watermarked_texts_ids, output_logprobs = [], [], []
        output_group = outputs[0].outputs
        for o in output_group:
            text = o.text
            text_ids = o.token_ids
            # try:
            #     assert self.watermark_tokenizer(text, return_tensors='pt', add_special_tokens=False)['input_ids'].shape[1] == len(text_ids)
            #     assert len(text_ids) == len(o.logprobs)
            # except Exception as e:
            #     import pdb; pdb.set_trace()
            #     print(f"Error: {e}")

            logprobs = [lp[id].logprob for lp, id in zip(o.logprobs, text_ids)]
            watermarked_texts.append(text) 
            watermarked_texts_ids.append(text_ids)
            output_logprobs.append(logprobs)

        # print('done')
        return watermarked_texts, watermarked_texts_ids, output_logprobs

    def get_per_token_logps(self, original_text, watermarked_texts_ids):
        '''
        Compute the log probabilities of the watermarked text given the original text.
        '''
        # get G/R split
        green_red_split = self._get_green_red_split(original_text)

        # concatenate the original text and the watermarked text
        ## add prompt instruction
        messages = [
            {
                "role": "system", "content": SYS_PROMPT,
            },
            {
                "role": "user",  "content": original_text
            },
        ]
        prompt = self.watermark_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        ## tokenize
        prompt = self.watermark_tokenizer(prompt, return_tensors='pt').to(self.watermark_model.device)

        all_logprobs = []
        for wm_text_ids in watermarked_texts_ids:
            watermarked = torch.tensor(wm_text_ids).unsqueeze(0).to(self.watermark_model.device)
            watermarked_attention_mask = torch.ones(watermarked.size()).to(self.watermark_model.device)
            ## concatenate
            input_ids = torch.cat((prompt['input_ids'], watermarked), dim=1)
            attention_mask = torch.cat((prompt['attention_mask'],watermarked_attention_mask), dim=1)
            logits_to_keep = watermarked.size(1)  # only need to compute the logits for the watermarked tokens

            per_token_logps = self._get_per_token_logps(self.watermark_model, green_red_split, input_ids, attention_mask, logits_to_keep)
            per_token_logps = per_token_logps.squeeze(0)
            all_logprobs.append(per_token_logps)
        
        return all_logprobs

    def _get_per_token_logps(self, model, green_red_split, input_ids, attention_mask, logits_to_keep):
        # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
        logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
        # add waterark logits bias
        logits = watermark_logits_bias(logits, green_red_split, self.delta, self.alpha, self.measure_threshold)
        logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
        input_ids = input_ids[:, -logits_to_keep:]
        logits = logits[:, -logits_to_keep:]
        return selective_log_softmax(logits, input_ids)  # compute logprobs for the input tokens

    def _get_green_red_split(self, text):
        input_ids = self.embed_map_tokenizer(
            text,
            return_tensors='pt',
            truncation=True,  # Truncate input to the model's max length
            max_length=512    # Ensure the max length is 512 for RoBERTa
        ).to(self.embed_map_model.device)
        # last hidden states shape is [batch_size, sequence_length, hidden_size]
        outputs = self.embed_map_model(**input_ids, return_dict=True, sent_emb=True)
        mapping = outputs.pooler_output.squeeze()
        mapping = sign_ste(mapping)
        mapping = (mapping + 1) / 2
        green_red_split = mapping[self.mapping_list].clone().to(self.gpu1)
        return green_red_split

    def _next_token_entropy(self, logits):
        # logits = self.watermark_model(input_ids)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        mask = probs > 0
        entropy = -torch.sum(probs[mask] * torch.log(probs[mask]))
        return entropy

    def detect(self, text):
        if not text:  # empty text
            return None
        
        green_red_split = self._get_green_red_split(text).detach()  # the advantage term in the original policy gradient shouldn't have gradient

        input_ids = self.watermark_tokenizer.encode(
            text, 
            return_tensors='pt',
            add_special_tokens=False
        ).to(self.embed_map_model.device)

        logits = self.watermark_model(input_ids=input_ids, logits_to_keep=input_ids.size(1)).logits
        logits = logits[:, :-1, :].squeeze(0)  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred

        score = []
        for i, token_id in enumerate(input_ids[0]):
            if i <= self.measure_threshold:
                s = green_red_split[token_id]
                score.append(s)
            else:
                measure_entropy = self._next_token_entropy(logits[i - 1, :])  # first token doesn't have logits
                if measure_entropy >= self.alpha:
                    s = green_red_split[token_id]
                    score.append(s)

        # check gradient
        score = torch.stack(score)  # [seq_len]
        normalized_score = sum(score)/len(score)
        # normalized_score = normalized_score.item()
        return normalized_score

    def compute_rewards(self, data, watermarked_texts):
        """
        Compute the rewards for the generated watermarked texts.

        Args:
            data (dict): A data entry from the original dataset.
            watermarked_text (list): The generated watermarked texts.

        Returns:
            list: A list of computed reward values for each watermarked_text.
        """
        # relevance & text quality
        relevance_scores, text_quality_scores = [], []
        for watermarked_text in watermarked_texts:
            text_quality, relevance = None, None
            quality_result_dict = _judge_text_quality(data['original'], watermarked_text, model='gpt-4o')
            if quality_result_dict:
                text_quality = quality_result_dict['Text quality']
                relevance = quality_result_dict['Relevance']
            relevance_scores.append(relevance)
            text_quality_scores.append(text_quality)
        ## fill in the None values
        relevance_scores_filled = fill_na(relevance_scores)
        text_quality_scores_filled = fill_na(text_quality_scores)
        ## normalize the scores
        relevance_scores_normalized = (relevance_scores_filled - np.min(relevance_scores_filled)) / (np.max(relevance_scores_filled) - np.min(relevance_scores_filled) + 1e-8)
        text_quality_scores_normalized = (text_quality_scores_filled - np.min(text_quality_scores_filled)) / (np.max(text_quality_scores_filled) - np.min(text_quality_scores_filled) + 1e-8)

        # detectability
        detect_ori, detect_wm = [], []
        detect_para, detect_senti, detect_senti_latter, detect_hate = [], [], [], []
        
        original_score = self.detect(data['original'])
        detect_ori.append(original_score)
        for wm_text in watermarked_texts:
            # detect
            wm_score = self.detect(wm_text)
            detect_wm.append(wm_score)

            # paraphrase attack (may fail)
            para_wm_text = paraphrase_attack(wm_text, max_call=1, model='gpt-4o')
            para_wm_score = self.detect(para_wm_text)
            detect_para.append(para_wm_score)

            # sentiment spoofing attack (may fail)
            senti_result_dict = spoofing_attack(wm_text, max_call=1, model='gpt-4o')
            senti_wm_text = senti_result_dict['spoofing_watermarked_text']
            senti_wm_score = self.detect(senti_wm_text)
            detect_senti.append(senti_wm_score)

            # latter sentiment spoofing attack (may fail)
            original_sentiment = senti_result_dict['original_sentiment']
            target_modified_sentiment = senti_result_dict['target_modified_sentiment']
            latter_senti_result_dict = latter_spoofing_attack(wm_text, original_sentiment, target_modified_sentiment, max_call=1, model='gpt-4o')
            latter_senti_wm_text = latter_senti_result_dict['latter_spoofing_watermarked_text']
            latter_senti_wm_score = self.detect(latter_senti_wm_text)
            detect_senti_latter.append(latter_senti_wm_score)

            # hate spoofing attack
            hate_wm_text = hate_attack(self.hate_phrases_list, wm_text)
            hate_wm_score = self.detect(hate_wm_text)
            detect_hate.append(hate_wm_score)

        ## fill in the None values
        detect_para_filled = fill_na(detect_para)
        detect_senti_filled = fill_na(detect_senti)
        detect_senti_latter_filled = fill_na(detect_senti_latter)
        # gather the detectability scores
        d1, d2 = [], []
        for d_wm in detect_wm:
            tmp = - detect_ori[0] + d_wm
            d1.append(tmp.item())
        for d_para, d_senti, d_senti_latter, d_hate in zip(detect_para_filled, detect_senti_filled, detect_senti_latter_filled, detect_hate):
            d_neg = (d_senti + d_senti_latter + d_hate.item()) / 3
            d2.append(+ d_para - d_neg)
        ## normalize the scores
        d1_normalized = (d1 - np.min(d1)) / (np.max(d1) - np.min(d1) + 1e-8)
        d2_normalized = (d2 - np.min(d2)) / (np.max(d2) - np.min(d2) + 1e-8)

        # overall reward
        rewards = []
        for d1, d2, r, q in zip(d1_normalized, d2_normalized, relevance_scores_normalized, text_quality_scores_normalized):
            reward = d1 + d2 + r + q
            rewards.append(reward)

        G = len(watermarked_texts)  # debug
        result_dict = {
            'rewards': rewards,
            'relevance_scores': [s for s in relevance_scores if s is not None],
            'text_quality_scores': [s for s in text_quality_scores if s is not None],
            'detect_ori': detect_ori,
            'detect_wm': detect_wm,
            'detect_para': [d for d in detect_para if d is not None],
            'detect_senti': [d for d in detect_senti if d is not None],
            'detect_senti_latter': [d for d in detect_senti_latter if d is not None],
            'detect_hate': [d for d in detect_hate if d is not None],
            # =======debug======== #
            'sucess_para': len([d for d in detect_para if d is not None]) / G,
            'sucess_senti': len([d for d in detect_senti if d is not None]) / G,
            'sucess_senti_latter': len([d for d in detect_senti_latter if d is not None]) / G,
        }

        return result_dict



if __name__ == "__main__":
    os.environ["VLLM_USE_V1"] = "0"
    os.environ["VLLM_DISABLE_PROGRESS_BAR"] = "true"

    args = tyro.cli(Args)
    args.minibatch_size = int(args.batch_size // args.num_minibatches)
    run_name = f"batch{args.batch_size}-nmini{args.num_minibatches}-G{args.G}-V2"

    # make checkpoint dir and init best reward
    args.checkpoint_dir = rf"/blue/buyuheng/li_an.ucsb/projects/rl-watermark/ckpts/batch{args.batch_size}-nmini{args.num_minibatches}-G{args.G}-V2"
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    best_mean_reward = float("-inf")  # track best

    if args.track:
        import wandb

        wandb.init(
            project=args.wandb_project_name,
            entity=args.wandb_entity,
            config=vars(args),
            name=run_name,
            save_code=True,
        )

    # TRY NOT TO MODIFY: seeding
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cudnn.deterministic = args.torch_deterministic

    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")

    # env setup
    actor = Actor(args.embed_map_model_name, args.watermark_model_name)
    optimizer = optim.Adam(actor.parameters(), lr=args.learning_rate, eps=1e-5)
    train_set = load_dataset(args.dataset_name, split='train')
    if args.is_sanity_check:
        # use only one batch for sanity check
        train_set = train_set.select(range(args.batch_size))

    # TRY NOT TO MODIFY: start the game
    global_step = 0
    start_time = time.time()


    for iteration in tqdm(range(1, args.num_iterations + 1), desc="Training iterations"):
        # Annealing the rate if instructed to do so.  TODO
        if args.anneal_lr:
            frac = 1.0 - (iteration - 1.0) / args.num_iterations
            lrnow = frac * args.learning_rate
            optimizer.param_groups[0]["lr"] = lrnow

        for i in range(0, len(train_set), args.batch_size):
            batch = train_set[i : i + args.batch_size]

            # rollout before optimization
            all_watermarked_texts, all_watermarked_text_ids, all_logprobs = [], [], []  # [B, G]
            ## rollout
            for data_idx in tqdm(range(len(batch['original'])), desc="Rolling out one batch"):
                data = {k: batch[k][data_idx] for k in batch.keys()}
                with torch.no_grad():
                    watermarked_texts, watermarked_texts_ids, logprobs = actor.rollout(data['original'], args.G)
                all_watermarked_texts.append(watermarked_texts)
                all_watermarked_text_ids.append(watermarked_texts_ids)
                logprobs = [torch.tensor(lg) for lg in logprobs]  # [G, seq_len_i]
                all_logprobs.append(logprobs)
            
            ## compute rewards
            all_rewards = []  # [B, G]
            all_rewards_relevance, all_rewards_text_quality, all_rewards_detect = [], [], []  # [B*G]
            all_rewards_detect_ori, all_rewards_detect_wm = [], []
            all_rewards_detect_para, all_rewards_detect_senti, all_rewards_detect_senti_latter,  all_rewards_detect_hate = [], [], [], []
            # ==========debug======== #
            all_success_para, all_success_senti, all_success_senti_latter = [], [], []
            for data_idx in tqdm(range(len(batch['original'])), desc="Computing rewards"):
                data = {k: batch[k][data_idx] for k in batch.keys()}
                watermarked_texts = all_watermarked_texts[data_idx]

                result_dict = actor.compute_rewards(data, watermarked_texts)  # [G]
                all_rewards.append(result_dict['rewards'])
                all_rewards_relevance.extend(result_dict['relevance_scores'])
                all_rewards_text_quality.extend(result_dict['text_quality_scores'])
                all_rewards_detect_ori.extend(result_dict['detect_ori'])
                all_rewards_detect_wm.extend(result_dict['detect_wm'])
                all_rewards_detect_para.extend(result_dict['detect_para'])
                all_rewards_detect_senti.extend(result_dict['detect_senti'])
                all_rewards_detect_senti_latter.extend(result_dict['detect_senti_latter'])
                all_rewards_detect_hate.extend(result_dict['detect_hate'])
                # ==========debug======== #
                all_success_para.append(result_dict['sucess_para'])
                all_success_senti.append(result_dict['sucess_senti'])
                all_success_senti_latter.append(result_dict['sucess_senti_latter'])

            ## save checkpoint with best reward
            if global_step >0:
                flat_rews = [r for sub in all_rewards for r in sub]  # flatten rewards
                mean_reward = np.mean(flat_rews)
                if mean_reward > best_mean_reward:
                    best_mean_reward = mean_reward
                    ckpt_path = os.path.join(args.checkpoint_dir, "embed_map_model_best")
                    # save the embed_map model + tokenizer
                    actor.embed_map_model.save_pretrained(ckpt_path)
                    actor.embed_map_tokenizer.save_pretrained(ckpt_path)
                    print(f"[Checkpoint] new best mean reward {mean_reward:.4f}, saved to {ckpt_path} after step {global_step}")

            if global_step != -1:
                # record detailed rewards
                print(
                    f"Step: {global_step}, "
                    f"relevance: {np.mean(all_rewards_relevance):.4f}, "
                    f"text_quality: {np.mean(all_rewards_text_quality):.4f}, "
                    f"detect_ori: {torch.mean(torch.stack(all_rewards_detect_ori)).item():.4f}, "
                    f"detect_wm: {torch.mean(torch.stack(all_rewards_detect_wm)).item():.4f}, "
                    f"detect_para: {torch.mean(torch.stack(all_rewards_detect_para)).item():.4f}, "
                    f"detect_senti: {torch.mean(torch.stack(all_rewards_detect_senti)).item():.4f}, "
                    f"detect_senti_latter: {torch.mean(torch.stack(all_rewards_detect_senti_latter)).item():.4f}, "
                    f"detect_hate: {torch.mean(torch.stack(all_rewards_detect_hate)).item():.4f}, "
                    , flush=True
                )
                wandb.log({
                    "train/reward/relevance_scores": np.mean(all_rewards_relevance),
                    "train/reward/text_quality_scores": np.mean(all_rewards_text_quality),
                    "train/reward/detect_ori": torch.mean(torch.stack(all_rewards_detect_ori)).item(),
                    "train/reward/detect_wm": torch.mean(torch.stack(all_rewards_detect_wm)).item(),
                    "train/reward/detect_para": torch.mean(torch.stack(all_rewards_detect_para)).item(),
                    "train/reward/detect_senti": torch.mean(torch.stack(all_rewards_detect_senti)).item(),
                    "train/reward/detect_senti_latter": torch.mean(torch.stack(all_rewards_detect_senti_latter)).item(),
                    "train/reward/detect_hate": torch.mean(torch.stack(all_rewards_detect_hate)).item(),
                    #==========debug======== #
                    "train/success_rate_para": np.mean(all_success_para),
                    "train/success_rate_senti": np.mean(all_success_senti),
                    "train/success_rate_senti_latter": np.mean(all_success_senti_latter),
                }, step=global_step)

            b_logprobs = all_logprobs  # [B, G, seq_len_i]
            b_rewards = torch.tensor(all_rewards)  # [B, G]

            # normalize rewards to get advantages
            mean = b_rewards.mean(dim=1, keepdim=True)
            std = b_rewards.std(dim=1, keepdim=True) + 1e-8
            b_advantages = (b_rewards - mean) / std

            # Optimizing the policy and value network
            b_inds = np.arange(args.batch_size)
            np.random.shuffle(b_inds)  # shuffle the batch indices

            for start in range(0, args.batch_size, args.minibatch_size):
                # get minibatch
                end = start + args.minibatch_size
                mb_inds = b_inds[start:end]

                mb_original_text = [batch['original'][idx] for idx in mb_inds]  # [mb_size]
                mb_watermarked_text_ids = [all_watermarked_text_ids[idx] for idx in mb_inds]  # [mb_size, G]
                mb_logprobs = [b_logprobs[idx] for idx in mb_inds]  # [mb_size, G, seq_len_i]
                mb_advantages = b_advantages[mb_inds]  # [mb_size, G]

                new_mb_logprobs = []  # [mb_size, G, seq_len_i]
                for original_text, watermarked_texts_ids in zip(mb_original_text, mb_watermarked_text_ids):
                    new_logprobs = actor.get_per_token_logps(original_text, watermarked_texts_ids)  # [G, seq_len_i]
                    new_logprobs = new_logprobs  # [G, seq_len_i]
                    new_mb_logprobs.append(new_logprobs)
                
                loss, total_output_len = 0, 0
                for j in range(len(new_mb_logprobs)):  # iterate through minibatch
                    for i in range(args.G):  # iterate through group
                        new_logprobs = new_mb_logprobs[j][i]  # [seq_len_i]
                        old_logprobs = mb_logprobs[j][i]
                        old_logprobs = old_logprobs.to(new_logprobs.device)
                        try:
                            # assert len(new_logprobs) == len(old_logprobs)
                            ratio = torch.exp(new_logprobs - old_logprobs)
                        except Exception as e:
                            print(e)
                            print(j, i)
                            import pdb; pdb.set_trace()
                            print(e)
                        pg_loss = -mb_advantages[j][i] * ratio
                        loss += pg_loss.sum()
                        total_output_len += len(new_logprobs)
                
                loss /= total_output_len  # average over the total output length
                wandb.log({"train/loss": loss.item()}, step=global_step)

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(actor.parameters(), args.max_grad_norm)
                optimizer.step()

                global_step += 1

                # TRY NOT TO MODIFY: record rewards for plotting purposes
                # writer.add_scalar("learning_rate", optimizer.param_groups[0]["lr"], global_step)
                print("Step:", global_step, "loss:", loss.item())
