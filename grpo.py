# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy
import os
import random
import time
from dataclasses import dataclass

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import tyro
from torch.utils.tensorboard import SummaryWriter

from transformers import AutoTokenizer, AutoModelForCausalLM
from vllm import LLM, SamplingParams
from datasets import load_dataset

from models_cl import RobertaForCL
from util import vocabulary_mapping, WatermarkLogitsBias, selective_log_softmax, sign_ste, watermark_logits_bias
from text_quality_score import _judge_text_quality

@dataclass
class Args:
    exp_name: str = os.path.basename(__file__)[: -len(".py")]
    """the name of this experiment"""
    seed: int = 666
    """seed of the experiment"""
    torch_deterministic: bool = True
    """if toggled, `torch.backends.cudnn.deterministic=False`"""
    cuda: bool = True
    """if toggled, cuda will be enabled by default"""
    track: bool = True
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "rl-watermark"
    """the wandb's project name"""
    wandb_entity: str = None
    """the entity (team) of wandb's project"""

    # GRPO training arguments
    num_iterations: int = 200
    """the number of iterations (computed in runtime)"""
    batch_size: int = 4
    """the batch size"""
    num_minibatches: int = 1
    """the number of mini-batches"""
    eval_interval: int = 5
    """the interval of evaluation"""
    G: int = 2
    """the number of rollouts generated for each original text"""
    learning_rate: float = 2.5e-4
    """the learning rate of the optimizer"""
    anneal_lr: bool = False
    """Toggle learning rate annealing for policy and value networks"""
    clip_coef: float = 0.2
    """the surrogate clipping coefficient"""
    max_grad_norm: float = 0.5
    """the maximum norm for the gradient clipping"""
    # target_kl: float = None
    # """the target KL divergence threshold"""

    # Watermark specific arguments
    embed_map_model_name: str = "Shiyu-Lab/roberta-base-watermark-embed"
    """the name of the embedding model"""
    watermark_model_name: str = "meta-llama/Llama-3.1-8B-Instruct"
    """the name of the watermark model"""

    # Dataset specific arguments
    dataset_name: str = "Shiyu-Lab/C4-contrastive-watermark"
    """the name of the dataset"""
    num_paraphrased_llama: int = None
    """the number of paraphrased examples generated by Llama"""
    num_paraphrased_gpt: int = None
    """the number of paraphrased examples generated by GPT"""
    num_sentiment_spoof: int = None
    """the number of sentiment spoofing examples"""
    num_latter_sentiment_spoof: int = None
    """the number of latter sentiment spoofing examples"""
    num_hate: int = None
    """the number of hate spoofing examples"""

    # Sanity check arguments
    is_sanity_check: bool = True
    """if toggled, this experiment will be a sanity check"""

    # to be filled in runtime
    minibatch_size: int = 0
    """the mini-batch size (computed in runtime)"""

SYS_PROMPT = f'''Paraphrase the following text while preserving its original meaning. Ensure that the output meets the following criteria:

1. **Preserves Meaning** – The paraphrase should convey the same core idea without omitting or distorting information.
2. **Fluency and Grammar** – The paraphrase must be natural, grammatically correct, and well-structured.
3. **Appropriate Length** – Maintain a similar length unless a slight adjustment improves clarity.
4. **Consistency with Context** – Retain the original tone and formality (e.g., academic, casual, professional).
5. **Minimal Redundancy** – Avoid unnecessary repetition while keeping essential details.
6. **Retains Nuances** – Preserve connotations, implied meanings, and idiomatic expressions where appropriate.

Just provide the paraphrased version of the text, without any introductory or concluding phrases.
'''


class Actor(nn.Module):
    def __init__(self, embed_map_model_name, watermark_model_name):
        super().__init__()
        cuda_visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES", None)
        self.gpu0 = torch.device(f"cuda:{cuda_visible_devices.split(',')[0]}")  # for wm model - transformer
        self.gpu1 = torch.device(f"cuda:{cuda_visible_devices.split(',')[1]}")  # for wm model - vllm
        # self.gpu2 = torch.device(f"cuda:{cuda_visible_devices.split(',')[2]}")  # for embed model
        os.environ["VLLM_USE_V1"] = "0"

        self.watermark_model_vllm = LLM(
            model="meta-llama/Llama-3.1-8B-Instruct", 
            tensor_parallel_size=1,
            max_model_len=1000,
        )

        self.embed_map_tokenizer = AutoTokenizer.from_pretrained(embed_map_model_name)
        self.embed_map_model = RobertaForCL.from_pretrained(embed_map_model_name).to(self.gpu1)
        for param in self.embed_map_model.parameters():
            param.requires_grad = True
        self.watermark_tokenizer = AutoTokenizer.from_pretrained(watermark_model_name)
        self.watermark_model = AutoModelForCausalLM.from_pretrained(watermark_model_name).to(self.gpu1)
        for param in self.watermark_model.parameters():
            param.requires_grad = False  # freeze the watermark model

        vocabulary_size = self.watermark_model.config.vocab_size
        self.mapping_list = vocabulary_mapping(vocabulary_size, 384, seed=66)

        self.delta = 0.13  # watermark strength
        self.alpha = 2.0  # entropy threshold to add watermark
        self.measure_threshold = 20  # threshold to measure the entropy of the logits


    def rollout(self, text, G):
        # get G/R split
        green_red_split = self._get_green_red_split(text)

        # add prompt instruction
        messages = [
            {
                "role": "system", "content": SYS_PROMPT,
            },
            {
                "role": "user",  "content": text
            },
        ]
        prompt = self.watermark_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        # generate watermarked text
        logits_processors = [WatermarkLogitsBias(green_red_split, self.alpha, self.delta)]
        sampling_params = SamplingParams(
            n=G,
            top_p=0.9,
            max_tokens=500,
            logprobs=0,  # only return logprobs for the generated tokens
            logits_processors=logits_processors,
        )
        outputs = self.watermark_model_vllm.generate([prompt], sampling_params)
        ## save output results
        watermarked_texts, output_logprobs = [], []
        output_group = outputs[0].outputs
        for o in output_group:
            text = o.text
            text_ids = o.token_ids
            logprobs = [lp[id].logprob for lp, id in zip(o.logprobs, text_ids)]
            watermarked_texts.append(text)
            output_logprobs.append(logprobs)

        return watermarked_texts, output_logprobs

    def get_per_token_logps(self, original_text, watermarked_texts):
        '''
        Compute the log probabilities of the watermarked text given the original text.
        '''
        # get G/R split
        green_red_split = self._get_green_red_split(original_text)

        # concatenate the original text and the watermarked text
        ## add prompt instruction
        messages = [
            {
                "role": "system", "content": SYS_PROMPT,
            },
            {
                "role": "user",  "content": original_text
            },
        ]
        prompt = self.watermark_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        ## tokenize
        prompt = self.watermark_tokenizer(prompt, return_tensors='pt').to(self.watermark_model.device)

        all_logprobs = []
        for wm_text in watermarked_texts:
            watermarked = self.watermark_tokenizer(wm_text, return_tensors='pt').to(self.watermark_model.device)
            ## concatenate
            input_ids = torch.cat((prompt['input_ids'], watermarked['input_ids']), dim=1)
            attention_mask = torch.cat((prompt['attention_mask'], watermarked['attention_mask']), dim=1)
            logits_to_keep = watermarked['input_ids'].size(1)  # only need to compute the logits for the watermarked tokens

            per_token_logps = self._get_per_token_logps(self.watermark_model, green_red_split, input_ids, attention_mask, logits_to_keep)
            per_token_logps = per_token_logps.squeeze(0)
            all_logprobs.append(per_token_logps)
        
        return all_logprobs


    def _get_per_token_logps(self, model, green_red_split, input_ids, attention_mask, logits_to_keep):
        # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
        logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
        # add waterark logits bias
        logits = watermark_logits_bias(logits, green_red_split, self.delta, self.alpha, self.measure_threshold)
        logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
        input_ids = input_ids[:, -logits_to_keep:]
        logits = logits[:, -logits_to_keep:]
        return selective_log_softmax(logits, input_ids)  # compute logprobs for the input tokens

    def _get_green_red_split(self, text):
        input_ids = self.embed_map_tokenizer(
            text,
            return_tensors='pt',
            truncation=True,  # Truncate input to the model's max length
            max_length=512    # Ensure the max length is 512 for RoBERTa
        ).to(self.embed_map_model.device)
        # last hidden states shape is [batch_size, sequence_length, hidden_size]
        outputs = self.embed_map_model(**input_ids, return_dict=True, sent_emb=True)
        mapping = outputs.pooler_output.squeeze()
        mapping = sign_ste(mapping)
        mapping = (mapping + 1) / 2
        green_red_split = mapping[self.mapping_list].clone().to(self.gpu1)
        return green_red_split

    def _next_token_entropy(self, logits):
        # logits = self.watermark_model(input_ids)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        mask = probs > 0
        entropy = -torch.sum(probs[mask] * torch.log(probs[mask]))
        return entropy

    def detect(self, text):
        green_red_split = self._get_green_red_split(text).detach()  # the advantage term in the original policy gradient shouldn't have gradient

        input_ids = self.embed_map_tokenizer.encode(
            text,
            return_tensors='pt',
        ).to(self.embed_map_model.device)

        logits = self.watermark_model(input_ids=input_ids, logits_to_keep=input_ids.size(1)).logits
        logits = logits[:, :-1, :].squeeze(0)  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred

        score = []
        for i, token_id in enumerate(input_ids[0]):
            if i <= self.measure_threshold:
                s = green_red_split[token_id]
                score.append(s)
            else:
                # measure_ids = input_ids[0][:i].clone().detach().unsqueeze(0)
                measure_entropy = self._next_token_entropy(logits[i - 1, :])  # first token doesn't have logits
                if measure_entropy >= self.alpha:
                    s = green_red_split[token_id]
                    score.append(s)

        # check gradient
        score = torch.stack(score)  # [seq_len]
        normalized_score = sum(score)/len(score)
        # normalized_score = normalized_score.item()
        return normalized_score

        
    def compute_rewards(self, data, watermarked_texts):
        """
        Compute the rewards for the generated watermarked texts.

        Args:
            data (dict): A data entry from the original dataset.
            watermarked_text (list): The generated watermarked texts.

        Returns:
            list: A list of computed reward values for each watermarked_text.
        """
        # relevance
        # complexity
        relevance_scores, text_quality_scores = [], []
        for watermarked_text in watermarked_texts:
            text_quality, relevance = 0, 0
            quality_result_dict = _judge_text_quality(data['original'], watermarked_text)
            if quality_result_dict:  # TODO: what if API call fails?
                text_quality = quality_result_dict['Text quality']
                relevance = quality_result_dict['Relevance']
            relevance_scores.append(relevance)
            text_quality_scores.append(text_quality)

        # detectability
        positive_scores, negative_scores = [], []
        negative_cname_prefixes = ['sentiment', 'latter_sentiment', 'hate']
        negative_cnames = [f'{p}_spoof_0' for p in negative_cname_prefixes]

        original_score = self.detect(data['original'])

        for cname, text in data.items():
            if cname == 'original':
                continue

            if cname.startswith('paraphrase'):
                score = self.detect(text)
                positive_scores.append(score)
            elif cname in negative_cnames:
                if text:
                    score = self.detect(text)
                    negative_scores.append(score)

        positive_score = sum(positive_scores) / len(positive_scores) if positive_scores else 0
        negative_score = sum(negative_scores) / len(negative_scores) if negative_scores else 0

        detectability_rewards = []
        for wm_text in watermarked_texts:
            wm_score = self.detect(wm_text)
            reward = - original_score + wm_score + positive_score - negative_score
            detectability_rewards.append(reward)
        
        rewards = []
        for d, r, q in zip(detectability_rewards, relevance_scores, text_quality_scores):
            reward = d + r + q
            rewards.append(reward)

        result_dict = {
            'rewards': rewards,
            'relevance_scores': relevance_scores,
            'text_quality_scores': text_quality_scores,
            'detectability_rewards': detectability_rewards,
        }

        return result_dict



if __name__ == "__main__":
    args = tyro.cli(Args)
    args.minibatch_size = int(args.batch_size // args.num_minibatches)
    run_name = f"wm__{args.exp_name}__{args.seed}"

    if args.track:
        import wandb

        wandb.init(
            project=args.wandb_project_name,
            entity=args.wandb_entity,
            sync_tensorboard=True,
            config=vars(args),
            name=run_name,
            save_code=True,
        )
    writer = SummaryWriter(f"runs/{run_name}")
    writer.add_text(
        "hyperparameters",
        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
    )

    # TRY NOT TO MODIFY: seeding
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cudnn.deterministic = args.torch_deterministic

    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")

    # env setup
    actor = Actor(args.embed_map_model_name, args.watermark_model_name)
    optimizer = optim.Adam(actor.parameters(), lr=args.learning_rate, eps=1e-5)
    train_set = load_dataset(args.dataset_name, split='train')
    if args.is_sanity_check:
        # use only one batch for sanity check
        train_set = train_set.select(range(args.batch_size))

    # TRY NOT TO MODIFY: start the game
    global_step = 0
    start_time = time.time()


    for iteration in range(1, args.num_iterations + 1):
        # Annealing the rate if instructed to do so.  TODO
        if args.anneal_lr:
            frac = 1.0 - (iteration - 1.0) / args.num_iterations
            lrnow = frac * args.learning_rate
            optimizer.param_groups[0]["lr"] = lrnow

        for i in range(0, len(train_set), args.batch_size):
            batch = train_set[i : i + args.batch_size]

            # rollout before optimization
            all_watermarked_text, all_logprobs, all_rewards = [], [], []  # [B, G]
            all_rewards_relevance, all_rewards_text_quality, all_rewards_detectability = [], [], []  # [B*G]
            for data_idx in range(len(batch['original'])):
                data = {k: batch[k][data_idx] for k in batch.keys()}
                with torch.no_grad():
                    watermarked_texts, logprobs = actor.rollout(data['original'], args.G)
                result_dict = actor.compute_rewards(data, watermarked_texts)  # [G]
                rewards = result_dict['rewards']  # [G]

                all_watermarked_text.append(watermarked_texts)
                logprobs = [torch.tensor(lg) for lg in logprobs]  # [G, seq_len_i]
                all_logprobs.append(logprobs)
                all_rewards.append(rewards)

                all_rewards_relevance.extend(result_dict['relevance_scores'])
                all_rewards_text_quality.extend(result_dict['text_quality_scores'])
                all_rewards_detectability.extend(result_dict['detectability_rewards'])

            if global_step != 0:
                # record detailed rewards
                wandb.log({
                    "relevance_scores": np.mean(all_rewards_relevance),
                    "text_quality_scores": np.mean(all_rewards_text_quality),
                    "detectability_rewards": np.mean(all_rewards_detectability),
                }, step=global_step)

            b_logprobs = all_logprobs  # [B, G, seq_len_i]
            b_rewards = torch.tensor(all_rewards)  # [B, G]

            # normalize rewards to get advantages
            mean = b_rewards.mean(dim=1, keepdim=True)
            std = b_rewards.std(dim=1, keepdim=True) + 1e-8
            b_advantages = (b_rewards - mean) / std

            # Optimizing the policy and value network
            b_inds = np.arange(args.batch_size)
            np.random.shuffle(b_inds)  # shuffle the batch indices

            for start in range(0, args.batch_size, args.minibatch_size):
                # get minibatch
                end = start + args.minibatch_size
                mb_inds = b_inds[start:end]

                mb_original_text = [batch['original'][idx] for idx in mb_inds]  # [mb_size]
                mb_watermarked_text = [all_watermarked_text[idx] for idx in mb_inds]  # [mb_size, G]
                mb_logprobs = [b_logprobs[idx] for idx in mb_inds]  # [mb_size, G, seq_len_i]
                mb_advantages = b_advantages[mb_inds]  # [mb_size, G]

                new_all_logprobs = []
                for original_text, watermarked_texts in zip(mb_original_text, mb_watermarked_text):
                    new_logprobs = actor.get_per_token_logps(original_text, watermarked_texts)  # [G, seq_len_i]
                    new_logprobs = new_logprobs  # [G, seq_len_i]
                    # new_logprobs = [torch.tensor(lg) for lg in new_logprobs]  # [G, seq_len_i]
                    new_all_logprobs.append(new_logprobs)
                
                new_mb_logprobs = new_all_logprobs  # [mb_size, G, seq_len_i]

                loss, total_output_len = 0, 0
                for j in range(len(new_mb_logprobs)):  # iterate through minibatch
                    for i in range(args.G):  # iterate through group
                        new_logprobs = new_mb_logprobs[j][i]  # [seq_len_i]
                        old_logprobs = mb_logprobs[j][i]
                        old_logprobs = old_logprobs.to(new_logprobs.device)
                        assert len(new_logprobs) == len(old_logprobs)
                        ratio = torch.exp(new_logprobs - old_logprobs)
                        pg_loss = -mb_advantages[j][i] * ratio
                        loss += pg_loss.sum()
                        total_output_len += len(new_logprobs)
                
                loss /= total_output_len  # average over the total output length
                wandb.log({"loss": loss.item()})

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(actor.parameters(), args.max_grad_norm)
                optimizer.step()

                global_step += 1

                # TRY NOT TO MODIFY: record rewards for plotting purposes
                writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
                writer.add_scalar("losses/policy_loss", loss.item(), global_step)
                print("loss:", loss.item())
                # print("SPS:", int(global_step / (time.time() - start_time)))
                # writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)

    writer.close()
